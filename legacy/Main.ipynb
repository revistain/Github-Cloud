{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 1: 모든 파일 복사 및 이름 변경\n",
    "python\n",
    "코드 복사\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import shutil\n",
    "import hashlib\n",
    "import datetime\n",
    "import filetype\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# image/video\n",
    "import imghdr\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "EXIFTOOL_PATH = \"./exiftool-13.11_64/exiftool.exe\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move all files to one folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files_with_folder_info(input_folder: str, output_folder: str) -> list:\n",
    "    copied_files = []\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # 전체 파일 개수 계산\n",
    "    total_files = sum(len(files) for _, _, files in os.walk(input_folder))\n",
    "\n",
    "    # tqdm을 사용하여 진행 상태 표시\n",
    "    with tqdm(total=total_files, desc=\"Copying files\", unit=\"file\") as pbar:\n",
    "        for root, dirs, files in os.walk(input_folder):\n",
    "            for file in files:\n",
    "                src_path = os.path.join(root, file)\n",
    "                \n",
    "                # 원본 폴더 경로 정보를 파일명에 포함\n",
    "                relative_path = os.path.relpath(root, input_folder)\n",
    "                folder_info = relative_path.replace(os.sep, \"-\")  # 경로 구분자를 '-'로 변경\n",
    "                new_filename = f\"{Path(file).stem}_folder-{folder_info}{Path(file).suffix}\"\n",
    "                \n",
    "                # 파일명 중복 처리\n",
    "                dest_path = Path(output_folder) / new_filename\n",
    "                counter = 1\n",
    "                while dest_path.exists():\n",
    "                    stem = dest_path.stem\n",
    "                    suffix = dest_path.suffix\n",
    "                    dest_path = Path(output_folder) / f\"{stem} ({counter}){suffix}\"\n",
    "                    counter += 1\n",
    "                \n",
    "                # 파일 복사\n",
    "                try:\n",
    "                    shutil.copy2(src_path, dest_path)\n",
    "                    copied_files.append(str(dest_path))\n",
    "                except Exception as e:\n",
    "                    print(f\"[ERROR] Failed to copy {src_path} to {dest_path}: {e}\")\n",
    "                \n",
    "                pbar.update(1)  # 진행 바 업데이트\n",
    "\n",
    "    return copied_files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete uselss files ( delete . and 0 bit files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_files(directory: str):\n",
    "    dir_path = Path(directory)  # Convert the string directory path to a Path object\n",
    "    \n",
    "    # Check if the directory exists\n",
    "    if not dir_path.is_dir():\n",
    "        print(f\"[ERROR] {directory} is not a valid directory.\")\n",
    "        return\n",
    "    \n",
    "    deleted_count = 0  # Counter for successfully deleted files\n",
    "    failed_count = 0   # Counter for failed deletions\n",
    "    processed_count = 0  # Counter for processed files\n",
    "    \n",
    "    # Iterate over all files in the directory (including subdirectories if needed)\n",
    "    for p in dir_path.iterdir():\n",
    "        try:\n",
    "            # Only process files (not directories)\n",
    "            if p.is_file():\n",
    "                processed_count += 1  # Increment processed files count\n",
    "                if p.stat().st_size == 0 or p.name.startswith(('.', '._')):  # Check for empty or unwanted files\n",
    "                    try:\n",
    "                        p.unlink()  # Delete the file\n",
    "                        deleted_count += 1  # Increment deleted count\n",
    "                        #print(f\"[DELETE] Removed {p}\")\n",
    "                    except Exception as e:\n",
    "                        failed_count += 1  # Increment failed deletion count\n",
    "                        print(f\"[ERROR] Failed to delete {p}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to process {p}: {e}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\nProcessing Summary:\")\n",
    "    print(f\"Total files processed: {processed_count}\")\n",
    "    print(f\"Files successfully deleted: {deleted_count}\")\n",
    "    print(f\"Files failed to delete: {failed_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix image extension if mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fix_image_extension_if_mismatch(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    This part exsist since filetype can not recognise json files.\n",
    "    filetype 라이브러리를 사용하여 파일 확장자가 실제 파일 형식과 일치하지 않는 경우 수정합니다.\n",
    "    **JPG 파일이 실제 JSON 데이터인 경우를 먼저 확인하여 .json 확장자로 변경하는 기능**을 포함합니다.\n",
    "    이미지, 동영상, 문서 등 다양한 파일 형식을 지원합니다.\n",
    "\n",
    "    Args:\n",
    "        file_path: 파일 경로\n",
    "\n",
    "    Returns:\n",
    "        수정된 파일 경로 (확장자가 변경되었거나, 변경이 없었다면 원래 경로)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        p = Path(file_path)\n",
    "        original_suffix_lower = p.suffix.lower()\n",
    "\n",
    "        # **1. JPG/JPEG 파일이 JSON인지 먼저 확인 (filetype 기반 함수에 통합)**\n",
    "        if original_suffix_lower == \".jpg\" or original_suffix_lower == \".jpeg\":\n",
    "            is_json = False\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    json.load(f)\n",
    "                is_json = True\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "            except UnicodeDecodeError:\n",
    "                pass\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            if is_json: # JSON 파일로 확인된 경우\n",
    "                json_ext = \".json\"\n",
    "                base_path = p.with_suffix(json_ext)\n",
    "                candidate = base_path\n",
    "                counter = 1\n",
    "                while candidate.exists():\n",
    "                    candidate = p.with_name(f\"{p.stem} ({counter}){json_ext}\")\n",
    "                    counter += 1\n",
    "                new_path = candidate\n",
    "\n",
    "                try:\n",
    "                    p.rename(new_path)\n",
    "                    #print(f\"[RENAME] JPG->JSON: {file_path} -> {new_path}\")\n",
    "                    return str(new_path) # JSON으로 변경 후 새 경로 반환\n",
    "                except Exception as e:\n",
    "                    print(f\"[ERROR] Rename failed (JPG->JSON): {file_path} -> {new_path}, {e}\")\n",
    "                    return file_path # JSON 리네임 실패 시 원래 경로 반환\n",
    "\n",
    "        # **2. filetype으로 파일 형식 감지 (JSON으로 변경되지 않은 경우)**\n",
    "        kind = filetype.guess(file_path)\n",
    "\n",
    "        if kind is not None: # 파일 형식을 감지한 경우\n",
    "            detected_extension = f\".{kind.extension}\"\n",
    "\n",
    "            if original_suffix_lower != detected_extension: # 확장자가 일치하지 않는 경우\n",
    "                base_path = p.with_suffix(detected_extension)\n",
    "                candidate = base_path\n",
    "                counter = 1\n",
    "                while candidate.exists(): # 새 파일 이름이 이미 존재하는 경우 숫자 증가\n",
    "                    candidate = p.with_name(f\"{p.stem} ({counter}){detected_extension}\")\n",
    "                    counter += 1\n",
    "                new_path = candidate\n",
    "\n",
    "                try:\n",
    "                    p.rename(new_path)\n",
    "                    #print(f\"[RENAME] Extension fixed using filetype: {file_path} -> {new_path}\")\n",
    "                    return str(new_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"[ERROR] Rename failed using filetype: {file_path} -> {new_path}, {e}\")\n",
    "                    return file_path # 리네임 실패 시 원래 경로 반환\n",
    "            else:\n",
    "                #print(f\"[INFO] Extension already matches detected file type (filetype): {file_path}\") # 확장자가 이미 일치하는 경우 정보 메시지\n",
    "                return file_path # 확장자가 이미 일치하므로 원래 경로 반환\n",
    "\n",
    "        else: # filetype.guess()가 파일 형식을 감지하지 못한 경우 (None 반환)\n",
    "            #print(f\"[WARNING] File type detection failed (filetype), keeping original extension: {file_path}\")\n",
    "            return file_path # 파일 형식 감지 실패 시 원래 경로 반환\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ERROR] File not found: {file_path}\") # 파일이 존재하지 않을 경우 에러 처리\n",
    "        return file_path\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] An error occurred with file (filetype): {file_path}, {e}\")\n",
    "        return file_path\n",
    "\n",
    "    return file_path # 예외 발생 없이, 확장자 변경도 없었을 경우 원래 경로 반환 (코드 흐름 상 도달 X, 명시적으로 추가)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Date from Json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datetime_from_json_sidecar(json_file_path: str) -> Optional[datetime.datetime]:\n",
    "    try:\n",
    "        import json\n",
    "        with open(json_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        ts_str = data.get(\"photoTakenTime\", {}).get(\"timestamp\")\n",
    "        if ts_str:\n",
    "            return datetime.datetime.fromtimestamp(int(ts_str))\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] get_datetime_from_json_sidecar: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date from file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_datetime_from_filename(filename: str, folder_path: str) -> Optional[datetime.datetime]:\n",
    "    \"\"\"\n",
    "    다양한 형식의 파일명에서 날짜/시간을 추출하는 함수.\n",
    "    - 우선순위: 파일명 패턴을 통해 날짜 추론.\n",
    "    - 마지막 수단: 파일명에 '_folder-'가 포함되어 있으면, 해당 정보로 'YYYY' 또는 'YYYYMM' 형태의 날짜를 추정.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1) 스크린샷 패턴: \"YYYY-MM-DD HH.MM.SS\"\n",
    "        m = re.search(r\"(\\d{4})-(\\d{2})-(\\d{2})\\s+(\\d{2})\\.(\\d{2})\\.(\\d{2})\", filename)\n",
    "        if m:\n",
    "            return datetime.datetime(*map(int, m.groups()))\n",
    "        \n",
    "        # 2) KakaoTalk_Photo 패턴: \"YYYY-MM-DD-HH-MM-SS\"\n",
    "        m = re.search(r\"(\\d{4})-(\\d{2})-(\\d{2})-(\\d{2})-(\\d{2})-(\\d{2})\", filename)\n",
    "        if m:\n",
    "            return datetime.datetime(*map(int, m.groups()))\n",
    "        \n",
    "        # 3) Resized 이미지 패턴: \"YYYYMMDD_HHMMSS\"\n",
    "        m = re.search(r\"(\\d{8})_(\\d{6})\", filename)\n",
    "        if m:\n",
    "            date_str, time_str = m.groups()\n",
    "            return datetime.datetime(\n",
    "                int(date_str[0:4]), int(date_str[4:6]), int(date_str[6:8]),\n",
    "                int(time_str[0:2]), int(time_str[2:4]), int(time_str[4:6])\n",
    "            )\n",
    "        \n",
    "        # 4) KakaoTalk_YYYYMMDD_HHMMSS 패턴\n",
    "        m = re.search(r\"KakaoTalk_(\\d{8})_(\\d{6})\", filename)\n",
    "        if m:\n",
    "            date_str, time_str = m.groups()\n",
    "            return datetime.datetime(\n",
    "                int(date_str[0:4]), int(date_str[4:6]), int(date_str[6:8]),\n",
    "                int(time_str[0:2]), int(time_str[2:4]), int(time_str[4:6])\n",
    "            )\n",
    "        \n",
    "        # 5) 문서, 명함, 영수증 등 \"YYYY-MM-DD_HHMMSS\" 패턴\n",
    "        m = re.search(r\"(\\d{4})-(\\d{2})-(\\d{2})_(\\d{6})\", filename)\n",
    "        if m:\n",
    "            y, mo, d, t = m.groups()\n",
    "            return datetime.datetime(\n",
    "                int(y), int(mo), int(d),\n",
    "                int(t[0:2]), int(t[2:4]), int(t[4:6])\n",
    "            )\n",
    "        \n",
    "        # 6) 간단히 YYYY-MM-DD 혹은 YYYY_MM_DD 추출\n",
    "        m = re.search(r\"(\\d{4})[-_](\\d{2})[-_](\\d{2})\", filename)\n",
    "        if m:\n",
    "            return datetime.datetime(*map(int, m.groups()))\n",
    "        \n",
    "        # 기존 형식에 매칭되지 않으면 MMDD 형식 (YYYYMMDD) 을 확인\n",
    "        m = re.search(r\"(\\d{4})(\\d{2})(\\d{2})\", filename)\n",
    "        if m:\n",
    "            year, month, day = map(int, m.groups())\n",
    "            # 시간 정보를 00:00:00 으로 설정하여 datetime 객체 생성\n",
    "            return datetime.datetime(year, month, day, 0, 0, 0)\n",
    "        \n",
    "        # === 마지막(최후의) 수단: _folder-YYYY 혹은 _folder-YYYYMM ===\n",
    "        m = re.search(r\"_folder-(\\d{4})(\\d{2})?\", filename)\n",
    "        if m:\n",
    "            year = int(m.group(1))\n",
    "            # 월이 없는 경우(YYYY만 있는 경우)는 1월로 처리\n",
    "            month = int(m.group(2)) if m.group(2) else 1\n",
    "            # 일은 확인할 수 없으므로 1일로 지정\n",
    "            return datetime.datetime(year, month, 1)\n",
    "        \n",
    "        # 아무 패턴에도 매칭되지 않을 경우 None\n",
    "        return None\n",
    "\n",
    "    except Exception as e:\n",
    "        # 원치 않는 예외가 발생하면 로그 남기고 None\n",
    "        # print(f\"[ERROR] extract_datetime_from_filename: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update metadata with datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_metadata_with_datetime(file_path: str, dt: datetime.datetime) -> str:\n",
    "    if dt is None:\n",
    "        return file_path\n",
    "    exiftool_datetime = dt.strftime(\"%Y:%m:%d %H:%M:%S\")\n",
    "    file_path_fixed = file_path.replace(\"\\\\\", \"/\")\n",
    "    original_filename = os.path.basename(file_path_fixed)\n",
    "    original_dir = os.path.dirname(file_path_fixed)\n",
    "    # 1. 임시 파일 이름 생성 (8자리 랜덤 숫자+알파벳)\n",
    "    while True:\n",
    "        random_filename = ''.join(random.choices(string.ascii_letters + string.digits, k=8))\n",
    "        temp_file_path = os.path.join(original_dir, random_filename)\n",
    "        if not os.path.exists(temp_file_path): # 2. 파일 이름 중복 확인\n",
    "            break\n",
    "    try:\n",
    "        # 3. 파일 이름 임시 변경\n",
    "        os.rename(file_path_fixed, temp_file_path)\n",
    "\n",
    "        cmd = [\n",
    "            EXIFTOOL_PATH,\n",
    "            f\"-AllDates={exiftool_datetime}\",\n",
    "            \"-overwrite_original\",\n",
    "            temp_file_path\n",
    "        ]\n",
    "        # Run the subprocess and handle Unicode errors if any\n",
    "        proc = subprocess.run(cmd, capture_output=True, text=True, errors=\"replace\")\n",
    "        if proc.returncode == 0:\n",
    "            # 5. 파일 이름 원래대로 복구 (성공 시)\n",
    "            os.rename(temp_file_path, file_path_fixed)\n",
    "            return file_path_fixed\n",
    "        else:\n",
    "            # 5. 파일 이름 원래대로 복구 (에러 발생 시에도 복구하여 원상태 유지)\n",
    "            print(f\"{original_filename} 파일에서 ExifTool 에러 발생: {proc.stderr.strip()}\") # 에러 메시지 출력\n",
    "            os.rename(temp_file_path, file_path_fixed)\n",
    "            return None # 에러 발생시 None 반환\n",
    "    except Exception as e:\n",
    "        print(f\"{original_filename} 파일 처리 중 일반 에러 발생: {e}\") # 에러 메시지 출력\n",
    "        # 5. 오류 발생 시 파일 이름이 변경되었을 수 있으므로, 원래대로 복구하는 안전 장치\n",
    "        if os.path.exists(temp_file_path) and not os.path.exists(file_path_fixed):\n",
    "            os.rename(temp_file_path, file_path_fixed) # 복구 시도\n",
    "        return None # 에러 발생시 None 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy all files to a single folder\n",
    "You can do it with out this, Its just for safety of your files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input(\"Top here and double check your input ouput dir(press enter)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"G:/1BASE/2PHOTO\"\n",
    "output_folder = \"G:/1BASE/2PHOTO Total\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_folder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m copied_files = copy_files_with_folder_info(\u001b[43minput_folder\u001b[49m, output_folder)\n",
      "\u001b[31mNameError\u001b[39m: name 'input_folder' is not defined"
     ]
    }
   ],
   "source": [
    "copied_files = copy_files_with_folder_info(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove 0 bit files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_files(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기 통계 변수\n",
    "\n",
    "\n",
    "# VALID_EXTENSIONS = {\n",
    "#     # 이미지 파일 확장자\n",
    "#     \".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\", \".heic\", \".tiff\", \".webp\",\n",
    "#     # 동영상 파일 확장자\n",
    "#     \".mp4\", \".mov\", \".avi\", \".mkv\", \".mts\", \".wmv\", \".3gp\", \".mpg\", \".mpeg\"\n",
    "# }\n",
    "\n",
    "# 처리 함수\n",
    "def process_folder(folder_path: str):\n",
    "    \n",
    "    extension_corrections = 0\n",
    "    json_attempts = 0\n",
    "    json_success = 0\n",
    "    filename_attempts = 0\n",
    "    filename_success = 0\n",
    "    metadata_updates = 0\n",
    "    total_files_processed = 0\n",
    "    \n",
    "    folder = Path(folder_path)\n",
    "    if not folder.is_dir():\n",
    "        print(f\"[ERROR] The path {folder_path} is not a valid directory.\")\n",
    "        return\n",
    "\n",
    "    # 폴더 내 모든 파일 처리\n",
    "    for file_path in tqdm(folder.rglob(\"*\"), desc=\"Processing metadata\", unit=\"file\"):\n",
    "        if not file_path.is_file():\n",
    "            continue  # 디렉토리 스킵\n",
    "\n",
    "        total_files_processed += 1\n",
    "        # if file_path.suffix.lower() not in VALID_EXTENSIONS:\n",
    "        #     continue  # 유효하지 않은 확장자는 스킵\n",
    "\n",
    "        # 확장자 교정 적용\n",
    "        old_path = str(file_path)\n",
    "        if file_path.suffix.lower() == \".json\":\n",
    "            continue  # JSON 파일은 스킵\n",
    "        else:\n",
    "            new_path = fix_image_extension_if_mismatch(old_path)\n",
    "            if new_path != old_path:\n",
    "                extension_corrections += 1\n",
    "            file_path = Path(new_path)\n",
    "\n",
    "            if new_path == \"garbage_file_deleted\":\n",
    "                continue\n",
    "\n",
    "            # Check if DateTimeOriginal OR MediaCreateDate exsist\n",
    "\n",
    "            try:\n",
    "                # 1) Check if DateTimeOriginal exists\n",
    "                cmd_dt = [EXIFTOOL_PATH, \"-DateTimeOriginal\", \"-s3\", str(file_path)]\n",
    "                proc_dt = subprocess.run(cmd_dt, capture_output=True, text=True, errors='replace')\n",
    "                dtoriginal = proc_dt.stdout.strip()\n",
    "\n",
    "                if dtoriginal:\n",
    "                    # If DateTimeOriginal is present, skip this file\n",
    "                    continue\n",
    "                \n",
    "                # 2) If no DateTimeOriginal, check *MediaCreateDate*\n",
    "                cmd_mcd = [EXIFTOOL_PATH, \"-MediaCreateDate\", \"-s3\", str(file_path)]\n",
    "                proc_mcd = subprocess.run(cmd_mcd, capture_output=True, text=True, errors='replace')\n",
    "                mediacreatedate = proc_mcd.stdout.strip()\n",
    "\n",
    "                if mediacreatedate:\n",
    "                    # If MediaCreateDate is present, skip this file\n",
    "                    continue\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed reading metadata for {file_path}: {e}\")\n",
    "                continue  # Optionally skip further processing on error\n",
    "\n",
    "\n",
    "            # JSON에서 날짜 추출\n",
    "            json_path = str(file_path) + \".json\"\n",
    "            dt = None\n",
    "            if os.path.isfile(json_path):\n",
    "                json_attempts += 1\n",
    "                dt = get_datetime_from_json_sidecar(json_path)\n",
    "                if dt:\n",
    "                    json_success += 1\n",
    "\n",
    "            # 파일명에서 날짜 추출\n",
    "            if dt is None:\n",
    "                filename_attempts += 1\n",
    "                dt = extract_datetime_from_filename(file_path.name, str(file_path.parent))\n",
    "                if dt:\n",
    "                    filename_success += 1\n",
    "            # 메타데이터 업데이트\n",
    "            if dt:\n",
    "                check = update_metadata_with_datetime(str(file_path), dt)\n",
    "                print(check)\n",
    "                metadata_updates += 1\n",
    "                if check == \"garbage_file_deleted\":\n",
    "                    continue\n",
    "                    \n",
    "    # 결과 출력\n",
    "    print(\"[INFO] Processing complete.\")\n",
    "    print(f\"Total files processed: {total_files_processed}\")\n",
    "    print(f\"Extension corrections made: {extension_corrections}\")\n",
    "    print(f\"JSON attempts: {json_attempts}, JSON successes: {json_success}\")\n",
    "    print(f\"Filename extraction attempts: {filename_attempts}, Filename extraction successes: {filename_success}\")\n",
    "    print(f\"Metadata updates attempted: {metadata_updates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_folder(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_folder_info(folder_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Removes '_folder-' suffix from filenames in the specified directory.\n",
    "    Handles naming conflicts by adding number suffixes.\n",
    "    \n",
    "    Args:\n",
    "        folder_path: String path to the directory to process\n",
    "    \"\"\"\n",
    "    folder = Path(folder_path)\n",
    "    \n",
    "    files = list(folder.iterdir())\n",
    "    for file_path in tqdm(files, desc=\"Processing Files\"):\n",
    "        if \"_folder-\" in file_path.name:\n",
    "            stem = file_path.stem\n",
    "            suffix = file_path.suffix\n",
    "            base_name_stem = re.sub(r\"_folder-.*\", \"\", stem)\n",
    "            base_name = base_name_stem + suffix\n",
    "            new_path = folder / base_name\n",
    "            \n",
    "            counter = 1\n",
    "            while new_path.exists():\n",
    "                new_path = folder / f\"{base_name_stem}_{counter}{suffix}\"\n",
    "                counter += 1\n",
    "            \n",
    "            file_path.rename(new_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove folder from name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_folder_info(output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import hashlib\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import uuid  # for random unique suffix if needed\n",
    "from tqdm import tqdm\n",
    "\n",
    "###############################################################################\n",
    "# 1) LOADING FILES (이미지 + 동영상)\n",
    "###############################################################################\n",
    "\n",
    "def load_files(folder_path):\n",
    "    \"\"\"\n",
    "    Recursively loads all image and video file paths from a folder (and subfolders).\n",
    "    \"\"\"\n",
    "    supported_images = ('.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff','heic')\n",
    "    supported_videos = ('.mp4', '.mov', '.avi', '.mkv', '.wmv', '.flv', '.webm')\n",
    "    \n",
    "    file_paths = []\n",
    "    \n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            lower_file = file.lower()\n",
    "            if lower_file.endswith(supported_images) or lower_file.endswith(supported_videos):\n",
    "                file_paths.append(os.path.join(root, file))\n",
    "    \n",
    "    return file_paths\n",
    "\n",
    "def is_video_file(filepath):\n",
    "    ext = os.path.splitext(filepath)[1].lower()\n",
    "    video_exts = ('.mp4', '.mov', '.avi', '.mkv', '.wmv', '.flv', '.webm')\n",
    "    return ext in video_exts\n",
    "\n",
    "def is_image_file(filepath):\n",
    "    ext = os.path.splitext(filepath)[1].lower()\n",
    "    image_exts = ('.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff','.heic')\n",
    "    return ext in image_exts\n",
    "\n",
    "###############################################################################\n",
    "# 2) IMAGE pHash & VIDEO SIGNATURE\n",
    "###############################################################################\n",
    "\n",
    "def compute_image_phash(image_path):\n",
    "    \"\"\"\n",
    "    Computes a perceptual hash (pHash) for an image using OpenCV.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            return None\n",
    "        image = cv2.resize(image, (8, 8), interpolation=cv2.INTER_AREA)\n",
    "        dct = cv2.dct(np.float32(image))\n",
    "        dct_roi = dct[0:8, 0:8]\n",
    "        median_val = np.median(dct_roi)\n",
    "        phash = ''.join('1' if px > median_val else '0'\n",
    "                        for row in dct_roi for px in row)\n",
    "        return phash\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def compute_video_signature(video_path):\n",
    "    \"\"\"\n",
    "    A naive signature for videos:\n",
    "      - file size (bytes)\n",
    "      - duration (seconds)\n",
    "      - pHash of the middle frame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        file_size = os.path.getsize(video_path)\n",
    "        \n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            return None\n",
    "        \n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "        if frame_count > 0 and fps > 0:\n",
    "            duration = frame_count / fps\n",
    "        else:\n",
    "            duration = 0\n",
    "        \n",
    "        # Middle frame\n",
    "        mid_index = int(frame_count // 2)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, mid_index)\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret or frame is None:\n",
    "            # fallback: first frame\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "            ret, frame = cap.read()\n",
    "        cap.release()\n",
    "        \n",
    "        if frame is not None:\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            gray = cv2.resize(gray, (8, 8), interpolation=cv2.INTER_AREA)\n",
    "            dct = cv2.dct(np.float32(gray))\n",
    "            dct_roi = dct[0:8, 0:8]\n",
    "            median_val = np.median(dct_roi)\n",
    "            phash = ''.join('1' if px > median_val else '0'\n",
    "                            for row in dct_roi for px in row)\n",
    "        else:\n",
    "            phash = None\n",
    "        \n",
    "        return {\n",
    "            'size': file_size,\n",
    "            'duration': duration,\n",
    "            'frame_phash': phash\n",
    "        }\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def compute_file_signature(filepath):\n",
    "    \"\"\"\n",
    "    For images, returns { 'type': 'image', 'hash': <pHash>, 'size': <bytes> }\n",
    "    For videos, returns { 'type': 'video', 'size': <bytes>, 'duration': <float>, 'frame_phash': <hash> }\n",
    "    \"\"\"\n",
    "    if is_image_file(filepath):\n",
    "        phash = compute_image_phash(filepath)\n",
    "        size = os.path.getsize(filepath)\n",
    "        return {\n",
    "            'type': 'image',\n",
    "            'hash': phash,\n",
    "            'size': size\n",
    "        }\n",
    "    elif is_video_file(filepath):\n",
    "        vinfo = compute_video_signature(filepath)\n",
    "        if vinfo is None:\n",
    "            return None\n",
    "        return {\n",
    "            'type': 'video',\n",
    "            'size': vinfo['size'],\n",
    "            'duration': vinfo['duration'],\n",
    "            'frame_phash': vinfo['frame_phash']\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "###############################################################################\n",
    "# 3) SHA-256 CHECK & SIMILARITY CALC\n",
    "###############################################################################\n",
    "\n",
    "def hamming_distance(h1, h2):\n",
    "    if not h1 or not h2:\n",
    "        return 64\n",
    "    return sum(ch1 != ch2 for ch1, ch2 in zip(h1, h2))\n",
    "\n",
    "def compute_sha256(file_path, chunk_size=65536):\n",
    "    \"\"\"\n",
    "    Computes SHA-256 hash for the file to check if bit-for-bit identical.\n",
    "    \"\"\"\n",
    "    sha = hashlib.sha256()\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            while True:\n",
    "                data = f.read(chunk_size)\n",
    "                if not data:\n",
    "                    break\n",
    "                sha.update(data)\n",
    "        return sha.hexdigest()\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_similarity(sig1, sig2, path1, path2, check_threshold):\n",
    "    \"\"\"\n",
    "    1) If same type (image-image or video-video), do a pHash-based (or video-based) comparison\n",
    "    2) If < check_threshold but file sizes are identical => do SHA-256 check => if identical, treat as 100%.\n",
    "    3) If different types => 0%\n",
    "    \"\"\"\n",
    "    if not sig1 or not sig2:\n",
    "        return 0.0\n",
    "    \n",
    "    if sig1['type'] != sig2['type']:\n",
    "        return 0.0\n",
    "    \n",
    "    # Image vs Image\n",
    "    if sig1['type'] == 'image' and sig2['type'] == 'image':\n",
    "        dist = hamming_distance(sig1['hash'], sig2['hash'])\n",
    "        phash_sim = (64 - dist) / 64 * 100\n",
    "        if phash_sim >= check_threshold:\n",
    "            return phash_sim\n",
    "        else:\n",
    "            # fallback: file size same => check SHA\n",
    "            if sig1['size'] == sig2['size']:\n",
    "                sha1 = compute_sha256(path1)\n",
    "                sha2 = compute_sha256(path2)\n",
    "                if sha1 and sha2 and sha1 == sha2:\n",
    "                    return 100.0\n",
    "            return phash_sim\n",
    "    \n",
    "    # Video vs Video\n",
    "    if sig1['type'] == 'video' and sig2['type'] == 'video':\n",
    "        ph1 = sig1.get('frame_phash')\n",
    "        ph2 = sig2.get('frame_phash')\n",
    "        if ph1 and ph2:\n",
    "            dist = hamming_distance(ph1, ph2)\n",
    "            frame_sim = (64 - dist) / 64 * 100\n",
    "        else:\n",
    "            frame_sim = 0\n",
    "        \n",
    "        size1, size2 = sig1['size'], sig2['size']\n",
    "        dur1, dur2 = sig1.get('duration', 0), sig2.get('duration', 0)\n",
    "        \n",
    "        # size similarity\n",
    "        if size1 == 0 or size2 == 0:\n",
    "            size_similarity = 0\n",
    "        else:\n",
    "            size_diff = abs(size1 - size2)\n",
    "            max_size = max(size1, size2)\n",
    "            size_penalty = (size_diff / max_size) * 100\n",
    "            size_similarity = 100 - size_penalty\n",
    "            if size_similarity < 0:\n",
    "                size_similarity = 0\n",
    "        \n",
    "        # duration similarity\n",
    "        if dur1 == 0 or dur2 == 0:\n",
    "            dur_similarity = 0\n",
    "        else:\n",
    "            dur_diff = abs(dur1 - dur2)\n",
    "            max_dur = max(dur1, dur2)\n",
    "            dur_penalty = (dur_diff / max_dur) * 100\n",
    "            dur_similarity = 100 - dur_penalty\n",
    "            if dur_similarity < 0:\n",
    "                dur_similarity = 0\n",
    "        \n",
    "        combined_sim = (frame_sim + size_similarity + dur_similarity) / 3\n",
    "        \n",
    "        if combined_sim >= check_threshold:\n",
    "            return combined_sim\n",
    "        else:\n",
    "            if size1 == size2 and size1 != 0:\n",
    "                sha1 = compute_sha256(path1)\n",
    "                sha2 = compute_sha256(path2)\n",
    "                if sha1 and sha2 and sha1 == sha2:\n",
    "                    return 100.0\n",
    "            return combined_sim\n",
    "    \n",
    "    return 0.0\n",
    "\n",
    "###############################################################################\n",
    "# 4) QUALITY + TIE-BREAK (NAME POLICY)\n",
    "###############################################################################\n",
    "\n",
    "def get_file_quality(sig):\n",
    "    \"\"\"\n",
    "    Images => file size\n",
    "    Videos => file_size + 1000 * duration\n",
    "    \"\"\"\n",
    "    if not sig:\n",
    "        return 0\n",
    "    if sig['type'] == 'image':\n",
    "        return sig['size']\n",
    "    elif sig['type'] == 'video':\n",
    "        return sig['size'] + 1000 * sig.get('duration', 0)\n",
    "    return 0\n",
    "\n",
    "def is_original_name(filename):\n",
    "    \"\"\"\n",
    "    Returns True if there's *no* sign of copy (like (1), 복사본, copy, etc.)\n",
    "    \"\"\"\n",
    "    lower = filename.lower()\n",
    "    # If \"copy\" or \"복사본\" is in name => false\n",
    "    if \"copy\" in lower or \"복사본\" in lower:\n",
    "        return False\n",
    "    # If there's a parenthesis with number => likely a copy\n",
    "    if \"(\" in lower and \")\" in lower:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def pick_lower_quality_or_tiebreak(path1, path2, sig1, sig2):\n",
    "    \"\"\"\n",
    "    If qualities differ => remove the lower one.\n",
    "    If tie => keep the file that looks like the 'original' name, otherwise random.\n",
    "    \"\"\"\n",
    "    q1 = get_file_quality(sig1)\n",
    "    q2 = get_file_quality(sig2)\n",
    "    \n",
    "    if q1 < q2:\n",
    "        return path1\n",
    "    elif q2 < q1:\n",
    "        return path2\n",
    "    else:\n",
    "        # tie => check name\n",
    "        orig1 = is_original_name(os.path.basename(path1))\n",
    "        orig2 = is_original_name(os.path.basename(path2))\n",
    "        if orig1 and not orig2:\n",
    "            return path2\n",
    "        elif orig2 and not orig1:\n",
    "            return path1\n",
    "        else:\n",
    "            return random.choice([path1, path2])\n",
    "\n",
    "###############################################################################\n",
    "# 5) MOVING DUPLICATES INSTEAD OF DELETING\n",
    "###############################################################################\n",
    "\n",
    "def move_file_to_duplicates(file_path, duplicates_folder):\n",
    "    \"\"\"\n",
    "    Moves the file to the 'duplicates_folder'. \n",
    "    If there's a collision, we append a random suffix to the filename.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(duplicates_folder):\n",
    "        os.makedirs(duplicates_folder, exist_ok=True)\n",
    "    \n",
    "    filename = os.path.basename(file_path)\n",
    "    destination = os.path.join(duplicates_folder, filename)\n",
    "    \n",
    "    # If a file with the same name already exists in duplicates_folder, rename\n",
    "    if os.path.exists(destination):\n",
    "        # E.g., insert a unique suffix: \"filename (uuid4).ext\"\n",
    "        name, ext = os.path.splitext(filename)\n",
    "        new_filename = f\"{name} ({uuid.uuid4().hex[:6]}){ext}\"\n",
    "        destination = os.path.join(duplicates_folder, new_filename)\n",
    "    \n",
    "    shutil.move(file_path, destination)\n",
    "    print(f\"Moved => {destination}\")\n",
    "\n",
    "###############################################################################\n",
    "# 6) MAIN LOGIC: DETECT & MOVE DUPLICATES\n",
    "###############################################################################\n",
    "\n",
    "def get_video_frame(video_path, fraction=0.5):\n",
    "    \"\"\"\n",
    "    Extract a frame at 'fraction' (0..1) of the video length for display\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        return None\n",
    "    frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    target = int(frame_count * fraction)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, target)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    if not ret:\n",
    "        return None\n",
    "    return frame\n",
    "\n",
    "def show_files_side_by_side(path1, path2, sig1, sig2, similarity):\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    print(f\"Similarity = {similarity:.2f}%\")\n",
    "    print(f\"File 1: {os.path.basename(path1)}  ({sig1['type']})\")\n",
    "    print(f\"File 2: {os.path.basename(path2)}  ({sig2['type']})\\n\")\n",
    "    \n",
    "    if sig1['type'] == 'image' and sig2['type'] == 'image':\n",
    "        img1 = Image.open(path1)\n",
    "        img2 = Image.open(path2)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        axes[0].imshow(img1)\n",
    "        axes[0].set_title(os.path.basename(path1))\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(img2)\n",
    "        axes[1].set_title(os.path.basename(path2))\n",
    "        axes[1].axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    elif sig1['type'] == 'video' and sig2['type'] == 'video':\n",
    "        frame1 = get_video_frame(path1, 0.5)\n",
    "        frame2 = get_video_frame(path2, 0.5)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        if frame1 is not None:\n",
    "            axes[0].imshow(cv2.cvtColor(frame1, cv2.COLOR_BGR2RGB))\n",
    "        axes[0].set_title(os.path.basename(path1))\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        if frame2 is not None:\n",
    "            axes[1].imshow(cv2.cvtColor(frame2, cv2.COLOR_BGR2RGB))\n",
    "        axes[1].set_title(os.path.basename(path2))\n",
    "        axes[1].axis('off')\n",
    "        plt.show()\n",
    "\n",
    "def detect_and_move_duplicates(folder_path, check_threshold=95.0, delete_threshold=99.0):\n",
    "    \"\"\"\n",
    "    - Scans the folder for images & videos, building signatures\n",
    "    - Pairwise compares them (O(n^2))\n",
    "    - If similarity >= delete_threshold => auto-move to 'duplicated_photos'\n",
    "    - If check_threshold <= similarity < delete_threshold => show user & ask\n",
    "      (options: y => move one file, n => keep, 1 => stop)\n",
    "    - If < check_threshold => skip\n",
    "    - Stats at the end\n",
    "    \"\"\"\n",
    "    all_files = load_files(folder_path)\n",
    "    total_files = len(all_files)\n",
    "    print(\"1/2\")\n",
    "    # Build signatures\n",
    "    signatures = {}\n",
    "    for f in tqdm(all_files):\n",
    "        signatures[f] = compute_file_signature(f)\n",
    "    \n",
    "    # Stats\n",
    "    stats = {\n",
    "        'total_files': total_files,\n",
    "        'pairs_compared': 0,\n",
    "        'duplicates_auto_moved': 0,\n",
    "        'duplicates_user_moved': 0,\n",
    "        'pairs_prompted': 0,\n",
    "        'pairs_skipped': 0,\n",
    "        'pairs_under_threshold': 0\n",
    "    }\n",
    "    \n",
    "    file_list = list(signatures.keys())\n",
    "    checked_pairs = set()\n",
    "    user_stopped = False\n",
    "    \n",
    "    # We'll move duplicates to a subfolder \"duplicated_photos\" inside folder_path\n",
    "    duplicates_folder = os.path.join(folder_path, \"duplicated_photos\")\n",
    "    print(\"2/2\")\n",
    "    for i in tqdm(range(len(file_list))):\n",
    "        if user_stopped:\n",
    "            break\n",
    "        for j in range(i+1, len(file_list)):\n",
    "            if user_stopped:\n",
    "                break\n",
    "            \n",
    "            path1 = file_list[i]\n",
    "            path2 = file_list[j]\n",
    "            \n",
    "            if (path1, path2) in checked_pairs or (path2, path1) in checked_pairs:\n",
    "                continue\n",
    "            checked_pairs.add((path1, path2))\n",
    "            \n",
    "            # If one was already moved (i.e. not in signatures anymore), skip\n",
    "            if path1 not in signatures or path2 not in signatures:\n",
    "                continue\n",
    "            \n",
    "            sig1 = signatures[path1]\n",
    "            sig2 = signatures[path2]\n",
    "            stats['pairs_compared'] += 1\n",
    "            \n",
    "            if not sig1 or not sig2:\n",
    "                continue\n",
    "            \n",
    "            similarity = get_similarity(sig1, sig2, path1, path2, check_threshold)\n",
    "            \n",
    "            if similarity >= delete_threshold:\n",
    "                # auto-move\n",
    "                to_move = pick_lower_quality_or_tiebreak(path1, path2, sig1, sig2)\n",
    "                print(f\"[AUTO] {similarity:.2f}% => Moving to duplicates folder: {to_move}\")\n",
    "                move_file_to_duplicates(to_move, duplicates_folder)\n",
    "                signatures.pop(to_move, None)\n",
    "                stats['duplicates_auto_moved'] += 1\n",
    "            \n",
    "            elif similarity >= check_threshold:\n",
    "                # prompt user\n",
    "                stats['pairs_prompted'] += 1\n",
    "                show_files_side_by_side(path1, path2, sig1, sig2, similarity)\n",
    "                \n",
    "                print(\"Options: [y] move one file, [n] keep both, [1] stop now\")\n",
    "                choice = input(\"Choice: \").strip().lower()\n",
    "                \n",
    "                if choice == 'y':\n",
    "                    to_move = pick_lower_quality_or_tiebreak(path1, path2, sig1, sig2)\n",
    "                    print(f\"[USER] {similarity:.2f}% => Moving: {to_move}\")\n",
    "                    move_file_to_duplicates(to_move, duplicates_folder)\n",
    "                    signatures.pop(to_move, None)\n",
    "                    stats['duplicates_user_moved'] += 1\n",
    "                elif choice == 'n':\n",
    "                    print(\"[SKIPPED] Kept both.\\n\")\n",
    "                    stats['pairs_skipped'] += 1\n",
    "                elif choice == '1':\n",
    "                    print(\"Stopping per user request...\")\n",
    "                    user_stopped = True\n",
    "                    break\n",
    "                else:\n",
    "                    # treat as 'n'\n",
    "                    print(\"[SKIPPED] Kept both.\\n\")\n",
    "                    stats['pairs_skipped'] += 1\n",
    "            \n",
    "            else:\n",
    "                stats['pairs_under_threshold'] += 1\n",
    "                # skip\n",
    "    \n",
    "    total_moved = stats['duplicates_auto_moved'] + stats['duplicates_user_moved']\n",
    "    files_remaining = total_files - total_moved\n",
    "    duplicates_found = total_moved + stats['pairs_skipped']\n",
    "    \n",
    "    print(\"\\n===== RUN SUMMARY =====\")\n",
    "    print(f\"Total files scanned: {stats['total_files']}\")\n",
    "    print(f\"Total pairs compared: {stats['pairs_compared']}\")\n",
    "    print(f\"Pairs < {check_threshold}% similarity: {stats['pairs_under_threshold']}\")\n",
    "    print(f\"Pairs prompted (≥ {check_threshold}% & < {delete_threshold}%): {stats['pairs_prompted']}\")\n",
    "    print(f\"  - Duplicates user moved: {stats['duplicates_user_moved']}\")\n",
    "    print(f\"  - Pairs user skipped (kept both): {stats['pairs_skipped']}\")\n",
    "    print(f\"Pairs auto-moved (≥ {delete_threshold}%): {stats['duplicates_auto_moved']}\")\n",
    "    print(f\"Total duplicates found (any ≥ {check_threshold}%): {duplicates_found}\")\n",
    "    print(f\"Total files moved: {total_moved}\")\n",
    "    print(f\"Files still remaining in original location: {files_remaining}\")\n",
    "    if user_stopped:\n",
    "        print(\"User stopped before checking all pairs.\")\n",
    "    else:\n",
    "        print(\"Completed all comparisons.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Example usage with user input for folder path.\n",
    "    \"\"\"\n",
    "    #folder_path = input(\"Put the folder directory here: \")\n",
    "    folder_path = input(output_folder)\n",
    "    detect_and_move_duplicates(folder_path, check_threshold=96.86, delete_threshold=96.87)\n",
    "\n",
    "main()  # Uncomment to run if you're in a normal Python environment\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gitexploit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
